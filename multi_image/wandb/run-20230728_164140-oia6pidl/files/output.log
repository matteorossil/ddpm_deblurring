Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f8937c003a0>
Traceback (most recent call last):
  File "/home/mr6744/my_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1478, in __del__
    self._shutdown_workers()
  File "/home/mr6744/my_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1442, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/mr6744/my_env/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1408593) is killed by signal: Aborted.
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/home/mr6744/my_env/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/mr6744/ddpm_deblurring/multi_image/trainer.py", line 504, in main
    trainer.run() # perform training
  File "/home/mr6744/ddpm_deblurring/multi_image/trainer.py", line 448, in run
    self.train()
  File "/home/mr6744/ddpm_deblurring/multi_image/trainer.py", line 382, in train
    denoiser_loss = self.diffusion.loss(residual, torch.cat((blur, flow_blur), dim=1))
  File "/home/mr6744/ddpm_deblurring/multi_image/ddpm.py", line 128, in loss
    eps_theta = self.eps_model(xt_, t)
  File "/home/mr6744/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mr6744/my_env/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/mr6744/my_env/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/home/mr6744/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mr6744/ddpm_deblurring/multi_image/denoiser.py", line 385, in forward
    return self.unet_forward(x, t)
  File "/home/mr6744/ddpm_deblurring/multi_image/denoiser.py", line 369, in unet_forward
    x = m(x, t)
  File "/home/mr6744/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mr6744/ddpm_deblurring/multi_image/denoiser.py", line 211, in forward
    x = self.res(x, t)
  File "/home/mr6744/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mr6744/ddpm_deblurring/multi_image/denoiser.py", line 93, in forward
    h = self.conv1(self.act1(x))
  File "/home/mr6744/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mr6744/ddpm_deblurring/multi_image/denoiser.py", line 10, in forward
    return x * torch.sigmoid(x)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 14.74 GiB total capacity; 12.90 GiB already allocated; 20.81 MiB free; 13.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/mr6744/my_env/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 76, in _wrap
    sys.exit(1)
  File "/home/mr6744/my_env/lib/python3.8/site-packages/wandb/sdk/lib/exit_hooks.py", line 36, in exit
    self._orig_exit(orig_code)  # type: ignore
SystemExit: 1
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/usr/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt